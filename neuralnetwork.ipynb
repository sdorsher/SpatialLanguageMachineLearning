{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import os, sys\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "file = open(\"airports-extended.dat.txt\",\"r\")\n",
    "import tensorflow as tf\n",
    "from bokeh.plotting import figure, output_notebook, show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is ahead of 'origin/master' by 5 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Changes not staged for commit:\n",
      "\tmodified:   mapping.ipynb\n",
      "\tmodified:   neuralnetwork.ipynb\n",
      "\tmodified:   neuralnetwork2.ipynb\n",
      "\n",
      "Untracked files:\n",
      "\t.ipynb_checkpoints/\n",
      "\tuntitled\n",
      "\tuntitled1\n",
      "\n",
      "no changes added to commit\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -m \"Saving my work at the end of the day dont know what I chnaged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def powerlaw(x,a,b,c):\n",
    "    return a*x**b+c\n",
    "\n",
    "def f5(x,a,b,c,d,g,h):\n",
    "    return a*x**5+b*x**4+c*x**3+d*x**2+g*x+h\n",
    "\n",
    "def f3(x,a,b,c,d):\n",
    "    return a*x**3+b*x**2+c*x+d\n",
    "\n",
    "\n",
    "def VowelToConsRatio(name):\n",
    "    \n",
    "    words = sum(c.isalpha() for c in name)\n",
    "    spaces = sum(c.isspace() for c in name)\n",
    "    others = len(name) - words - spaces\n",
    "    vowels = sum(map(name.lower().count, \"aeiou\"))\n",
    "    consonents = words - vowels\n",
    "\n",
    "\n",
    "    return float(vowels)/float(consonents), float(spaces)/float(consonents), float(others)/float(consonents)\n",
    "\n",
    "def CharCount(name):\n",
    "    \n",
    "    words = sum(c.isalpha() for c in name)\n",
    "    spaces = sum(c.isspace() for c in name)\n",
    "    others = len(name) - words - spaces\n",
    "    vowels = sum(map(name.lower().count, \"aeiou\"))\n",
    "    consonents = words - vowels\n",
    "\n",
    "\n",
    "    return int(consonents), int(vowels), int(spaces), int(others)\n",
    "\n",
    "\n",
    "\n",
    "def MergeCount(totalCount, thisCount):\n",
    "    totalCount2=totalCount+thisCount\n",
    "    return totalCount2\n",
    "\n",
    "\n",
    "\n",
    "def LatLongClassMaker(lat,long):\n",
    "    \n",
    "    if lat<30:\n",
    "        if long>-90:\n",
    "            return 1 #florida\n",
    "        elif long < -120:\n",
    "            return 2 #hawaii\n",
    "        else:\n",
    "            return 3 #texas\n",
    "    elif lat >50:\n",
    "        return 4 #alaska\n",
    "    elif long> -80 and lat < 40:\n",
    "        return 5 #south east\n",
    "    elif long> -80 and lat >40:\n",
    "        return 6 #new england\n",
    "    elif long< -80:\n",
    "        return int((50-lat)/20.*5)*int((125+long)/45.*9)+7\n",
    "    else:\n",
    "        return 0\n",
    "    #worst mapping ever-- ignores new england and city diversity\n",
    "    return 0\n",
    "ratio=[]\n",
    "coords=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=[]\n",
    "coords=[]\n",
    "charcountlist=[]\n",
    "region=[]\n",
    "file = open(\"airports-extended.dat.txt\",\"r\")\n",
    "for line in file:\n",
    "    line=re.split(',',line);\n",
    "\n",
    "    if line[3] == \"\\\"United States\\\"\":\n",
    "        thisratioV, thisratioS, thisratioO=VowelToConsRatio(line[1]);\n",
    "        ratio.append([thisratioV,thisratioS,thisratioO])\n",
    "        charcountlist.append(np.array([CharCount(line[1])]))\n",
    "        coords.append([float(line[6]),float(line[7])])\n",
    "        region.append(LatLongClassMaker(float(line[6]),float(line[7])))\n",
    "\n",
    "ratioarray=np.empty([len(ratio),3])\n",
    "coordsarray=np.empty([len(coords),2])\n",
    "regionarray=np.empty([len(coords)])\n",
    "charcountarray=np.empty([len(coords),4])\n",
    "regionarray=np.array(region)\n",
    "\n",
    "for i, rat in enumerate(ratio):\n",
    "    ratioarray[i,:]=rat\n",
    "for i, charct in enumerate(charcountlist):\n",
    "    charcountarray[i,:]=charct\n",
    "for i,coor in enumerate(coords):\n",
    "    coordsarray[i,:]=coor\n",
    "#    outputcoor=LatLongClassMaker(coor)\n",
    "#    regionarray[i]=outputcoor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testphrases=[\"Ufda!\", \"I just dont know about that\", \"Howdy, pardner\", \"Where are we?\", \"I thank whatever gods may be for my unconquerable soul.\", \"I welcome our new robot overlords\", \"l33t h4k3rs\", \"I know you I walked with you once upon a dream\", \"These stories dont mean anything if you have no one to tell them to\", \"Baby you have the sort of hands that rip me apart\",\"Multi-messenger astronomy\", \"Rainbow flag\", \"I prefer They or He?\", \"numerical relativity\", \"LIGO\", \"scalar field\", \"Osculating Orbits\", \"Monte-Carlo Simulation\", \"Data Analysis\", \"Data Science\", \"Parallelization\", \"Paralyzation\", \"Partial disability\", \"Non-epileptic seizures\", \"Wednesday Lunch\", \"Tuesday Lunch\", \"Thursday Lunch\", \"Guild Wars\", \"Elvenar\", \"Good Apple\"]\n",
    "\n",
    "\n",
    "colleaguephrases=[\"general relativity\", \"black hole\", \"loop quantum gravity\",  \"quantization\", \"space-time\", \"Hamiltonian constraint\", \"Ashtekar\", \"LiSA\", \"LIGO\", \"group\", \"white hole\", \"scalar field\", \"numerical relativity\", \"cosmology\", \"diffeomorphism\", \"continuum limit\", \"David Berger\", \"Reisner-Nordstrom\", \"black hole spacetime\", \"initial data\", \"interpretation of quantum mechanics\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ... 2 9 7]\n"
     ]
    }
   ],
   "source": [
    "print(regionarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ufda!\n",
      "I just dont know about that\n",
      "Howdy, pardner\n",
      "Where are we?\n",
      "I thank whatever gods may be for my unconquerable soul.\n",
      "I welcome our new robot overlords\n",
      "l33t h4k3rs\n",
      "I know you I walked with you once upon a dream\n",
      "These stories dont mean anything if you have no one to tell them to\n",
      "Baby you have the sort of hands that rip me apart\n",
      "Multi-messenger astronomy\n",
      "Rainbow flag\n",
      "I prefer They or He?\n",
      "numerical relativity\n",
      "LIGO\n",
      "scalar field\n",
      "Osculating Orbits\n",
      "Monte-Carlo Simulation\n",
      "Data Analysis\n",
      "Data Science\n",
      "Parallelization\n",
      "Paralyzation\n",
      "Partial disability\n",
      "Non-epileptic seizures\n",
      "Wednesday Lunch\n",
      "Tuesday Lunch\n",
      "Thursday Lunch\n",
      "Guild Wars\n",
      "Elvenar\n",
      "Good Apple\n",
      "general relativity\n",
      "black hole\n",
      "loop quantum gravity\n",
      "quantization\n",
      "space-time\n",
      "Hamiltonian constraint\n",
      "Ashtekar\n",
      "LiSA\n",
      "LIGO\n",
      "group\n",
      "white hole\n",
      "scalar field\n",
      "numerical relativity\n",
      "cosmology\n",
      "diffeomorphism\n",
      "continuum limit\n",
      "David Berger\n",
      "Reisner-Nordstrom\n",
      "black hole spacetime\n",
      "initial data\n",
      "interpretation of quantum mechanics\n",
      "[[1.0, 0.0, 0.5], [0.5714285714285714, 0.35714285714285715, 0.0], [0.3333333333333333, 0.1111111111111111, 0.1111111111111111], [1.0, 0.4, 0.2], [0.6071428571428571, 0.32142857142857145, 0.03571428571428571], [0.75, 0.3125, 0.0], [0.0, 0.16666666666666666, 0.6666666666666666], [0.8947368421052632, 0.5263157894736842, 0.0], [0.6875, 0.40625, 0.0], [0.56, 0.4, 0.0], [0.5333333333333333, 0.06666666666666667, 0.06666666666666667], [0.5714285714285714, 0.14285714285714285, 0.0], [0.6666666666666666, 0.4444444444444444, 0.1111111111111111], [0.7272727272727273, 0.09090909090909091, 0.0], [1.0, 0.0, 0.0], [0.5714285714285714, 0.14285714285714285, 0.0], [0.6, 0.1, 0.0], [0.8181818181818182, 0.09090909090909091, 0.09090909090909091], [0.7142857142857143, 0.14285714285714285, 0.0], [0.8333333333333334, 0.16666666666666666, 0.0], [0.875, 0.0, 0.0], [0.7142857142857143, 0.0, 0.0], [0.7, 0.1, 0.0], [0.8181818181818182, 0.09090909090909091, 0.09090909090909091], [0.4, 0.1, 0.0], [0.5, 0.125, 0.0], [0.3, 0.1, 0.0], [0.5, 0.16666666666666666, 0.0], [0.75, 0.0, 0.0], [0.8, 0.2, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "xphrases=[]\n",
    "for phrase in testphrases:\n",
    "    print(phrase)\n",
    "    vtc, stc, otc = VowelToConsRatio(phrase)\n",
    "    xphrases.append([vtc,stc,otc])\n",
    "xcolleague=[]\n",
    "for phrase in colleaguephrases:\n",
    "    print(phrase)\n",
    "    vtc, stc, otc = VowelToConsRatio(phrase)\n",
    "    xcolleague.append([vtc,stc,otc])\n",
    "\n",
    "print(xphrases)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = max(regionarray)\n",
    "n_char=np.max(charcountarray,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 27.,  8.,  6.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_char_labels=int(sum(n_char))\n",
    "n_char_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add 50 for each row of charcountarray to pad the onehotencoder\n",
    "charcountarray[1,:]+=50\n",
    "charcountarray[2,:]+=100\n",
    "charcountarray[3,:]+=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_enc = tf.one_hot(charcountarray.flatten(), depth=n_char_labels)\n",
    "y_true_enc = tf.one_hot(regionarray.flatten(), depth=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(9176), Dimension(76)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2294), Dimension(39)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_true_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9176/2294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9176, 2294]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e594e92824f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/data3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/data3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/data3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9176, 2294]"
     ]
    }
   ],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(x_enc,y_true_enc,test_size=0.33,shuffle=False, random_state=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainshp=np.shape(ytrain)\n",
    "testshp=np.shape(ytest)\n",
    "trainshp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_enc = tf.one_hot(xtrain.flatten(), depth=n_labels)\n",
    "y_true_enc = tf.one_hot(ytrain.flatten(), depth=n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain_flat = ytrain.flatten()\n",
    "ytest_flat=ytest.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layers=20 #20 lines of division is my biased theory\n",
    "hidden_size=3 #enough to draw a line with an offset I think, plus an little variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "git add neuralnetwork.ipynb\n",
    "git commit neuralnetwork.ipynb -m \"first attempt at neural network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W1 = tf.Variable(tf.random_normal([len(x), hidden_size], seed=42), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.zeros([hidden_size]), name=\"bias1\")\n",
    "hidden = tf.matmul(x, W1) + b1\n",
    "for i in np.arange(hidden_layers):\n",
    "\n",
    "    W = tf.Variable(tf.random_normal([hidden_size, hidden_size]), name=\"weight\"+str(i+2))\n",
    "    b = tf.Variable(tf.random_normal([hidden_size]), name=\"bias\"+str(i+2))\n",
    "    hidden = tf.matmul(hidden, W) + b\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([hidden_size, 1]), name=\"finalweight\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"finalbias\")\n",
    "y = tf.matmul(hidden, W2) + b2\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=ytrain, \n",
    "                                                              labels=y_label))\n",
    "train = tf.train.GradientDescentOptimizer(1e-1).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(tf.nn.sigmoid(ytrain) > 0.5, np.float32)\n",
    "#accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,y_label),np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in range(3000):\n",
    "    sess.run(train,\n",
    "             feed_dict={x: xtrain, y_label: ytrain})\n",
    "    if i % 300 == 0:\n",
    "        print sess.run([loss, accuracy], \n",
    "                       feed_dict={x: xtrain, y_label: ytrain}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
